<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Regression</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">First Draft</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="manag.html">Data Managment</a>
</li>
<li>
  <a href="visual.html">Data Visualization</a>
</li>
<li>
  <a href="reg.html">Regression</a>
</li>
<li>
  <a href="DT.html">Decission Tree</a>
</li>
<li>
  <a href="other.html">Other Data analytics</a>
</li>
<li>
  <a href="modeval.html">Model Evaluation</a>
</li>
<li>
  <a href="EnsLear.html">Ensemble Learning</a>
</li>
<li>
  <a href="mysql.html">mySQL</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Regression</h1>

</div>


<div id="introduction" class="section level1">
<h1><strong>introduction</strong></h1>
<p>First supervized learning. Variable dépendante disponible. Prédiction sur base d’autres variables</p>
</div>
<div id="linear-regression" class="section level1">
<h1><strong>Linear regression</strong></h1>
<ul>
<li><strong>Model</strong> : <span class="math inline">\(Y=\alpha + X \beta + \epsilon\)</span>
<ul>
<li>Linéaire: on suppose distribition normal</li>
<li><span class="math inline">\(\alpha\)</span> :intercepte : la reponse moyenne si les variables explicatives sont zéro</li>
</ul></li>
<li><strong>Remarque</strong>
<ul>
<li>Categorical data : set to as factor</li>
<li>Check Missing value : delete, impute, new catégorie</li>
</ul></li>
<li><strong>Hypothèses</strong> :
<ul>
<li><span class="math inline">\(rang(X) = p\)</span> =&gt; Rang est connu, exclus la multicolinéarité</li>
<li>X est une matrice déterminée</li>
<li><span class="math inline">\(\epsilon\)</span> sont des erreurs indépendantes</li>
<li><span class="math inline">\(E(\epsilon) = 0\)</span> =&gt; erreur de moyenne nulle (normalité des résidus)</li>
<li><span class="math inline">\(var(\epsilon) = \sigma_2 In\)</span> =&gt; variance Homoskédastique non autocorrélé</li>
</ul></li>
<li><strong>Estimation et propriétés des estimateurs</strong> : Estimation par moindres carrés ordinaires : Minimise les squares error. Estimateur le plus efficace dans la classe des estimateurs non biaisé :BLUE
<ul>
<li><span class="math inline">\(E[Y] = X \beta\)</span></li>
<li><span class="math inline">\(Var(Y) = \sigma In\)</span></li>
<li><span class="math inline">\(E[\hat(\beta)] = \beta\)</span></li>
<li><span class="math inline">\(var(\hat(\beta)) = \sigma (X&#39;X)^(-1)\)</span></li>
<li>Si <span class="math inline">\(\epsilon ~ N(0, \sigma In)\)</span>, alors <span class="math inline">\(\hat(\beta) ~ N(\beta, \sigma^2 (X&#39;X)^(-1))\)</span></li>
</ul></li>
</ul>
<p>De plus<br />
<span class="math display">\[SSTO = SSR + SSE\]</span> <span class="math display">\[\sum{(Y_i - \bar{Y})^2} = \sum{(\hat{Y}_i - \bar{Y})^2} + \sum{(Y_i - \hat{Y})^2}\]</span></p>
<ul>
<li><strong>Diagnostiques</strong> :
<ul>
<li><em>F-test</em> :
<ul>
<li><span class="math inline">\(H_0 : \beta_i = 0 \forall i\)</span></li>
<li>stat de test : <span class="math inline">\(\frac{(SSTO - SSE)/(p-1)}{SSE/(n-p)} = \frac{MSR}{MSE} \sim F(p-1,n-1)\)</span></li>
</ul></li>
<li><em>coefficient de détermination multiple <span class="math inline">\(R^2\)</span></em> : mesure de qualité d’ajustement
<ul>
<li><span class="math inline">\(\frac{SSR/SSTO}\)</span></li>
</ul></li>
<li><em>Multicolinéarité</em> : forte corrélation entre variables explicatives
<ul>
<li>Conséquence : Interprétation des coéfficients impossibles</li>
<li>Diagnostiques :
<ul>
<li>variance des coefficients très larges,</li>
<li>coefficients varient beaucoup a l’ajouts/retrait de variables,</li>
<li>coefficients ont signes non intruitifs</li>
<li>Calcule des VIF (variance inflation factor) : si mpoyenne des VIF &gt; 1 ou un VIF &gt;10)
<ul>
<li><span class="math inline">\(tolérance = 1-R²\)</span> et $ VIF = $</li>
</ul></li>
</ul></li>
<li>Solution : Supprimer des variables, regression de Ridge (permet l’inversion de la matrice X’X qui est impossible en cas de multicolinéarité parfaite)</li>
</ul></li>
<li>Linéarité : Graph des résidus Vs régresseurs
<ul>
<li>Si forme connue : transformer les regressieurs (log, sqrt) ou ajouté un terme (quadratique, log, d’interaction, …)</li>
</ul></li>
<li>Homoskédasticité : graph résidus vs valeurs prédites, test de Breush et Pagan, BreushPAgan, Berlett test, arch test
<ul>
<li>Variance des erreurs indépendante des variable explicative</li>
<li>Estimation reste correcte sous homoskédasticité : utilisé une variance corrigé : Régression de white</li>
</ul></li>
<li>Erreur Non indépendante : test d’autocorrélation Dubin watson test, plot acf
<ul>
<li>If résidual show definite relationship with prior résidual (like autocorrelation), the noise isn’t random and we still have some information that we can extract and put in the model</li>
<li>Problème de modèle : passer en log lin, oubli de régresseur (qui est autocorrélé), inclure des lag de la variable dépendante</li>
</ul></li>
<li>Normalité des erreurs : QQplot, test de Jarque Berra, KS test
<ul>
<li>estimation correcte mais interprétation des tests et des IC sont faussées car basé sur la normalité</li>
<li>théorie des grand nombre, si assez observations, estimateur OLS est assymptiquement normal et les test et IC tendent assymptotiquement</li>
</ul></li>
<li>Influential Point Analysis: Les valeurs abérantes peuvent crée des biais dans les estimateurs. Si trop extreme, on peut les deletes, check, impute, …
<ul>
<li>DFFITS</li>
<li>DFBETAS</li>
<li>Distance de Cooks :</li>
</ul></li>
</ul></li>
</ul>
<p><span class="math display">\[ D_i = \frac{e²_i}{s²p} [\frac{h_i}{(1-h_i)²}]\]</span> where <span class="math inline">\(s²= (n-p)^{-1}e^Te\)</span> est la moyenne des erreurs quadratiques de la regression. Et <span class="math inline">\(h_i =x^T(x^Tx)^{-1}\)</span>. Avec cutoff <span class="math inline">\(D_i &gt; 4/(n-k-1)\)</span> ou k est le nombre de paramètre</p>
<p>Distance de Cook mesure l’effet of deleting a given observation. Si supprimer des observations cause grosse influence, alors ce point est suppiser etre outlier.</p>
<ul>
<li><strong>Evaluation</strong> :
<ul>
<li>RMSE = sqrt(mean($residuals)^2) ou $residuals = actual-predicted</li>
</ul></li>
<li><strong>Interprétation</strong> :
<ul>
<li>Pour une augmentation de une unité de speed, dist augmente de 3.9324.</li>
<li>Intercepte donne la dist si speed vaut zero</li>
</ul></li>
</ul>
</div>
<div id="anova" class="section level1">
<h1><strong>ANOVA</strong></h1>
</div>
<div id="polynomiale-regression" class="section level1">
<h1><strong>Polynomiale regression</strong></h1>
<p>Si la relation entre variables explicatives et variable dépendante n’est pas linéaire.</p>
<p><span class="math display">\[ y_i = \alpha_0 + \alpha_i x_i + \alpha_2 x²_i+ ... + \epsilon_i\]</span> Possibilité dans des haut degré polynomials mais will cause overfitting.</p>
<p>Exemple :<br />
- Dependant variable = price of a commodity<br />
- Explicative variable = quantiée vendue The general principle is if the price is too cheap, people will not buy the commodity thinking it’s not of good quality, but if the price is too high, people will not buy due to cost consideration. Let’s try to quantify this relationship using linear and quadratic regression</p>
<pre class="r"><code>y &lt;-as.numeric(c(&quot;3.3&quot;,&quot;2.8&quot;,&quot;2.9&quot;,&quot;2.3&quot;,&quot;2.6&quot;,&quot;2.1&quot;,&quot;2.5&quot;,&quot;2.9&quot;,&quot;2.4&quot;,&quot;3.0&quot;,&quot;3.1&quot;,&quot;2.8&quot;,&quot;3.3&quot;,&quot;3.5&quot;,&quot;3&quot;))
x&lt;-as.numeric(c(&quot;50&quot;,&quot;55&quot;,&quot;49&quot;,&quot;68&quot;,&quot;73&quot;,&quot;71&quot;,&quot;80&quot;,&quot;84&quot;,&quot;79&quot;,&quot;92&quot;,&quot;91&quot;,&quot;90&quot;,&quot;110&quot;,&quot;103&quot;,&quot;99&quot;));

linear_reg &lt;-lm(y~x)
summary(linear_reg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.66844 -0.25994  0.03346  0.20895  0.69004 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 2.232652   0.445995   5.006  0.00024 ***
## x           0.007546   0.005463   1.381  0.19046    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3836 on 13 degrees of freedom
## Multiple R-squared:  0.128,  Adjusted R-squared:  0.06091 
## F-statistic: 1.908 on 1 and 13 DF,  p-value: 0.1905</code></pre>
<pre class="r"><code>plot(y)
lines(linear_reg$fitted.values)</code></pre>
<p><img src="reg_files/figure-html/reg%20poly-1.png" width="672" /></p>
<pre class="r"><code>quad_reg &lt;-lm(y~x +I(x^2) )
summary(quad_reg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x + I(x^2))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.43380 -0.13005  0.00493  0.20701  0.33776 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  6.8737010  1.1648621   5.901 7.24e-05 ***
## x           -0.1189525  0.0309061  -3.849  0.00232 ** 
## I(x^2)       0.0008145  0.0001976   4.122  0.00142 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2569 on 12 degrees of freedom
## Multiple R-squared:  0.6391, Adjusted R-squared:  0.5789 
## F-statistic: 10.62 on 2 and 12 DF,  p-value: 0.002211</code></pre>
<pre class="r"><code>plot(y)
lines(quad_reg$fitted.values)</code></pre>
<p><img src="reg_files/figure-html/reg%20poly-2.png" width="672" /></p>
<pre class="r"><code># improvement in R square, quadratic term significant</code></pre>
</div>
<div id="logistique-classification" class="section level1">
<h1><strong>Logistique : Classification</strong></h1>
<div id="general" class="section level3">
<h3>General</h3>
<ul>
<li><p><strong>Variable dépendante binaire</strong> : binomially distribued binomial distribution probability mass function : <span class="math inline">\(f(k;n,p) = P(X=k) = \left( \begin{array}{c} n \\ k \end{array} \right) p^k (1-p)^{n-k}\)</span></p></li>
<li><strong>Trois classe de modèle logistiques</strong>:
<ul>
<li><ol style="list-style-type: decimal">
<li>binomial logistic regression : var dépendante soit 0 soit 1</li>
</ol></li>
<li><ol start="2" style="list-style-type: decimal">
<li>multinomial logistic regression : 3 ouplus niveu pour la variable dépendante (on utilise ditribution multinomiale)</li>
</ol></li>
<li><ol start="3" style="list-style-type: decimal">
<li>ordered logistic regression</li>
</ol></li>
</ul></li>
<li><p><strong>Transformation logit</strong> : fonction de lien pour la regression : <span class="math inline">\(logit = \frac{e^t}{e^t+1}=\frac{1}{1+e^{-t}}\)</span></p></li>
<li><strong>LA cote</strong> : représente la relation entre presence/absence d’un event
<ul>
<li>odd = P(A)/(1-P(A))</li>
<li>un odd de 2 pour un event A mean l’event est deux fois plus probable qu’il se réalise que rien ne se réalise.</li>
<li>Odd Ratio : rapport des cotes = Odd(A) / Odd(B)</li>
<li>SI OR = 2 : Chanque que B se réalise sont deux fois suppérieur a celle de A</li>
</ul></li>
</ul>
</div>
<div id="binomial-logistic-model" class="section level3">
<h3>Binomial Logistic MODEL</h3>
<ul>
<li><p><strong>Model</strong> : <span class="math display">\[ logit(p_i) = \ln(\frac{p_i}{1-p_i}) = \beta_0 + \beta X \]</span></p></li>
<li><p><strong>Hypothèses</strong> :</p></li>
<li><p><strong>Estimation</strong> par MLE ou itérative avec optimisation du logLoss</p></li>
<li><strong>Diagnostiques</strong> :
<ul>
<li>Si but est classification : check les predictions et classement</li>
<li>Si but est analyse des coefficients : vérification des hypothèsese stat
<ul>
<li><p><strong>Wald test</strong> : same a t-test in reg lin. Test sur les levels des variables sont individuellements significatifs. Suit une distri chi-square.</p></li>
<li><p><strong>pseudo R-square</strong> : Mesure la proportion de variance expliqué par le modele. Mesure la différence entre la déviance un model null et fitted. Calcul par le likelihood ratio : <span class="math display">\[R²_i  = \frac{D_{null} - D_{fitted}}{D_{null}}\]</span> ou D est la déviance : $ D = - 2ln $</p></li>
<li><p><strong>Bivariate plot</strong> : observed and predictied vs variable explicative. Plot donne info sur comme le model sur comporte selon les différent niveau</p></li>
<li><p><strong>Matrice de classification</strong> : - Spécificity = combien de negatif le model prédit correctement - sensitivity = combien de positif le model prédit correctement</p></li>
</ul></li>
</ul></li>
</ul>
<pre class="r"><code>library(ggplot2)
library(mlbench)

  BreastCancer$Cl.thickness = as.numeric(as.character(BreastCancer$Cl.thickness))
  BreastCancer$IsMalignant = ifelse( BreastCancer$Class== &quot;benign&quot;, 0, 1)

  ggplot(data =BreastCancer, aes(x = Cl.thickness, y = IsMalignant)) +
    geom_jitter(height = 0.05, width = 0.3, alpha=0.4) +
    geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;))</code></pre>
<p><img src="reg_files/figure-html/reg%20log-1.png" width="672" /></p>
<pre class="r"><code>reglog =  glm(IsMalignant ~ Cl.thickness, family = &quot;binomial&quot;,
      data = BreastCancer)
summary(reglog)</code></pre>
<pre><code>## 
## Call:
## glm(formula = IsMalignant ~ Cl.thickness, family = &quot;binomial&quot;, 
##     data = BreastCancer)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1986  -0.4261  -0.1704   0.1730   2.9118  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -5.16017    0.37795  -13.65   &lt;2e-16 ***
## Cl.thickness  0.93546    0.07377   12.68   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 900.53  on 698  degrees of freedom
## Residual deviance: 464.05  on 697  degrees of freedom
## AIC: 468.05
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<pre class="r"><code>table(BreastCancer$Class, ifelse(predict(reglog, BreastCancer) &lt; 0.5, 0, 1))</code></pre>
<pre><code>##            
##               0   1
##   benign    453   5
##   malignant  94 147</code></pre>
</div>
<div id="multinomial-logistic-regression" class="section level3">
<h3>Multinomial Logistic Regression</h3>
<p>Variable dépendante a plus de une catégorie et suit une distribution multinomiale. On fait une regression logistic pour chaque classe et combine dans un seul equation sous contraint que la somme des probabilités vallent 1. Estimation par iterative optimization of the LogLoss function.</p>
<p>But : clairement de la classification. Deux méthode possible :<br />
- Pick de highest probability : classe dans la classe qui a le plus haute probabilité par rapport au autres classe. Méthode soufre de la “Class imbalance probleme” (si les classes sont non equilibré, tendance à toujours assigner dans la plus grande classe) - Ratio of probabilities : prendre la ratio des probabilité prédite et la prior distribution and choisir la classe basé sur le plus haut ratio. Cette méthode normalise les probabilité par le ratio du prior pour réduire le biais liéà la distribution du pior</p>
</div>
</div>
<div id="generalized-linear-models" class="section level1">
<h1><strong>Generalized Linear Models</strong></h1>
<p>Pour GLM, on suppose que la variable dépendante est issue de la famille de ditribution exponentielle incluant la normal, binomial, poisson, gamma, … etc. <span class="math display">\[ E(Y) = \mu = g^{-1}(X\beta) \]</span> In R : glm(formula, family=familytype(link=linkfunction), data=) - binomial, (link = “logit”) : modele logistique - gaussian, (link= “identity”) : modèle linéaire - Gamma, (link= “inverse”) : analyse de survie (time to failure of a machine in the industry) - poisson, (link = “log”) : How many calls will the call center receive today?</p>
</div>
<div id="model-selection" class="section level1">
<h1><strong>Model Selection</strong></h1>
<pre><code>*Stepwise : ajoute séquentielement la variables la plus significative. Après chaqeu ajout,le modèle réévalue la significativité des autres variables.</code></pre>
<p>Step : Model with 1 best feature, add next variables that maximise the evaluation function, … Proc?dure tr?s lourde. parfois necessaire d’utiliser FIlter m?thod avant.</p>
<pre class="r"><code>### Data prep ###
#################

## Data with best feature from Filter method
data = get(load(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/LoanDefaultPred.RData&quot;))
data[,&quot;default&quot;]=ifelse(data$loss ==0, 0,1)

data_model &lt;-na.omit(data[,c(&quot;id&quot;,&quot;f338&quot;,&quot;f422&quot;,&quot;f724&quot;,&quot;f636&quot;,&quot;f775&quot;,&quot;f222&quot;,&quot;f93&quot;,&quot;f309&quot;,&quot;f303&quot;,&quot;f113&quot;,&quot;default&quot;),])

### Forward ###
###############
full_model &lt;-glm(default ~f338 +f422 +f724 +f636 +f775 +f222 +f93 +f309+f303
                 +f113,data=data_model,family=binomial(link=&quot;logit&quot;))

null_model &lt;-glm(default ~1 ,data=data_model,family=binomial(link=&quot;logit&quot;))

forwards &lt;-step(null_model,scope=list(lower=formula(null_model),upper=formula(full_model)), direction=&quot;forward&quot;)</code></pre>
<pre><code>## Start:  AIC=11175.3
## default ~ 1
## 
##        Df Deviance   AIC
## + f422  1    11136 11140
## + f113  1    11150 11154
## + f222  1    11150 11154
## + f775  1    11165 11169
## + f93   1    11168 11172
## + f309  1    11171 11175
## + f303  1    11171 11175
## &lt;none&gt;       11173 11175
## + f636  1    11172 11176
## + f338  1    11173 11177
## + f724  1    11173 11177
## 
## Step:  AIC=11140.24
## default ~ f422
## 
##        Df Deviance   AIC
## + f113  1    11113 11119
## + f222  1    11114 11120
## + f775  1    11129 11135
## + f93   1    11131 11137
## &lt;none&gt;       11136 11140
## + f303  1    11135 11141
## + f309  1    11135 11141
## + f636  1    11135 11141
## + f338  1    11136 11142
## + f724  1    11136 11142
## 
## Step:  AIC=11118.59
## default ~ f422 + f113
## 
##        Df Deviance   AIC
## + f222  1    11096 11104
## + f775  1    11106 11114
## &lt;none&gt;       11113 11119
## + f93   1    11111 11119
## + f303  1    11112 11120
## + f636  1    11112 11120
## + f309  1    11112 11120
## + f338  1    11112 11120
## + f724  1    11113 11121
## 
## Step:  AIC=11103.78
## default ~ f422 + f113 + f222
## 
##        Df Deviance   AIC
## + f775  1    11090 11100
## &lt;none&gt;       11096 11104
## + f303  1    11095 11105
## + f636  1    11095 11105
## + f309  1    11095 11105
## + f93   1    11095 11105
## + f338  1    11096 11106
## + f724  1    11096 11106
## 
## Step:  AIC=11099.57
## default ~ f422 + f113 + f222 + f775
## 
##        Df Deviance   AIC
## + f303  1    11087 11099
## &lt;none&gt;       11090 11100
## + f309  1    11088 11100
## + f636  1    11089 11101
## + f93   1    11089 11101
## + f338  1    11090 11102
## + f724  1    11090 11102
## 
## Step:  AIC=11098.6
## default ~ f422 + f113 + f222 + f775 + f303
## 
##        Df Deviance   AIC
## &lt;none&gt;       11087 11099
## + f636  1    11086 11100
## + f93   1    11086 11100
## + f309  1    11086 11100
## + f338  1    11086 11100
## + f724  1    11087 11101</code></pre>
<pre class="r"><code>#best model with AIC criteria
formula(forwards)</code></pre>
<pre><code>## default ~ f422 + f113 + f222 + f775 + f303</code></pre>
</div>
<div id="regularization-algorithms" class="section level1">
<h1><strong>Regularization Algorithms</strong></h1>
<div id="ridge-regression" class="section level3">
<h3>Ridge regression</h3>
</div>
<div id="least-absolute-shrinkage-and-selection-operator-lasso" class="section level3">
<h3>Least Absolute Shrinkage and Selection Opérator LASSO</h3>
</div>
<div id="elastic-net" class="section level3">
<h3>Elastic Net</h3>
</div>
<div id="leas-angle-regression-lars" class="section level3">
<h3>Leas-Angle Regression LARS</h3>
<pre><code>  - **Lasso**</code></pre>
<p>dd penalty term against the complexity to reduce the degree of overfittingor the variance of the model by adding additional bas.</p>
<p>Check formul LASSO</p>
<p>Objective function for the penalized logistic regression: $ - [1/N y (_0 + x^T_t ) - (1 + ) ] + lambda[(1-)||||^2_2 ]$</p>
<pre><code>  - **ridge**</code></pre>
</div>
</div>
<div id="locally-estimated-scaterplot-smoothing-loess" class="section level1">
<h1><strong>Locally estimated Scaterplot Smoothing (LOESS)</strong></h1>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
