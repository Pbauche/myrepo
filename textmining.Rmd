---
title: "Text Mining"
---

# **Introduction**
Text mining = turn text into data analysis

Check : 
    - https://www.tidytextmining.com/index.html
    - Natural language processing (NLP)
    
# **Text summarization : Gong & Liu method (2001) via latent semantic analysis**
    - 1. Decompose the document D into individual sentences and use these sentences to form the candidate sentence set S and set k = 1.
    - 2. Construct the terms by sentences matrix A for the document D.
    - 3. Perform the SVD on A to obtain the singular value matrix, and the right singular vector matrix V^t. In the singular vector space, each sentence i is represented by the column vector.
    - 4. Select the k'th right singular vector from matrix V^t.
    - 5. Select the sentence that has the largest index value with the k'th right singular vector and include it in the summary.
    - 6. If k reaches the predefined number, terminate the operation otherwise, increment k by 1 and go back to Step 4


# **TF - IDF** 
    - Term frequency counts the number of occurrences of a term t in a document.
    - Inverse document frequency : $ idf = log_2 \frac{|D|}{|{d|t\in d}|} where |D| denotes the total number of documents and $|{d|t\in d}|$ is the number of documents where the term t appears
    
Certain terms that occur too frequently have little power in determining the reliance of a document. IDF weigh down the too frequently occurring word. (et inverserment)

A tf-idf matrix is a numerical representation of a collection of documents (represented by row) and words contained in it (represented by columns).

#  **Part of Speech (POS) tagging**
 This could help in classifying named entities in text into categories like persons, company, locations, expression of time, and so on.
 
# **Word Cloud**
The word cloud helps in visualizing the words most frequently being used in the reviews


```{r textmining}
library(data.table)
fine_food_data <-read.csv("C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/Food_Reviews.csv", stringsAsFactors =FALSE)
fine_food_data$Score <-as.factor(fine_food_data$Score)

head(fine_food_data[,10],2)

# Data preparation

library(caTools)
# Randomly split data and use only 10% of the dataset
set.seed(90)
split =sample.split(fine_food_data$Score, SplitRatio =0.10)
fine_food_data =subset(fine_food_data, split ==TRUE)
select_col <-c("Id","HelpfulnessNumerator","HelpfulnessDenominator","Score","Summary","Text")
fine_food_data_selected <-fine_food_data[,select_col]

dim(fine_food_data_selected)

# Summary text
##original
fine_food_data_selected[2,6]

  library(LSAfun)
genericSummary(fine_food_data_selected[2,6],k=1)
genericSummary(fine_food_data_selected[2,6],k=2)

# TF and IDF

library(tm)


fine_food_data_corpus <-VCorpus(VectorSource(fine_food_data_selected$Text))

## Standardize the text - Pre-Processing
fine_food_data_text_dtm <-DocumentTermMatrix(fine_food_data_corpus, 
                          control=list(tolower =TRUE,
                                       removeNumbers =TRUE,
                                       stopwords =TRUE,
                                       removePunctuation =TRUE,
                                       stemming =TRUE
                                       ))

#save frequently-appearing terms( more than 500 times) to a character vector
fine_food_data_text_freq <-findFreqTerms(fine_food_data_text_dtm, 500)

# create DTMs with only the frequent terms
fine_food_data_text_dtm <-fine_food_data_text_dtm[ , fine_food_data_text_freq]
tm::inspect(fine_food_data_text_dtm[1:5,1:10])

#Create a tf-idf matrix
fine_food_data_tfidf <-weightTfIdf(fine_food_data_text_dtm, normalize=FALSE)
tm::inspect(fine_food_data_tfidf[1:5,1:10])


#Part of Speech tagging

fine_food_data_corpus<-Corpus(VectorSource(fine_food_data_selected$Text[1:3]))
fine_food_data_cleaned <-tm_map(fine_food_data_corpus, PlainTextDocument)

##tolwer
fine_food_data_cleaned <-tm_map(fine_food_data_cleaned, tolower)

fine_food_data_cleaned <-tm_map(fine_food_data_cleaned, removeWords, stopwords("english"))

fine_food_data_cleaned <-tm_map(fine_food_data_cleaned,removePunctuation)

fine_food_data_cleaned <-tm_map(fine_food_data_cleaned, removeNumbers)

fine_food_data_cleaned <-tm_map(fine_food_data_cleaned, stripWhitespace)


library(openNLP)
library(NLP)

fine_food_data_string <-NLP::as.String(fine_food_data_cleaned[[1]])
sent_token_annotator <-Maxent_Sent_Token_Annotator()
word_token_annotator <-Maxent_Word_Token_Annotator()
fine_food_data_string_an <-annotate(fine_food_data_string, list(sent_token_annotator, word_token_annotator))

pos_tag_annotator <-Maxent_POS_Tag_Annotator()

fine_food_data_string_an2 <-annotate(fine_food_data_string, pos_tag_annotator, fine_food_data_string_an)

head(annotate(fine_food_data_string, Maxent_POS_Tag_Annotator(probs =TRUE), fine_food_data_string_an2))

fine_food_data_string_an2w <-subset(fine_food_data_string_an2, type == "word")
tags <-sapply(fine_food_data_string_an2w$features, `[[`, "POS")
table(tags)

plot(table(tags), type ="h", xlab="Part-Of_Speech", ylab ="Frequency")

head(sprintf("%s/%s", fine_food_data_string[fine_food_data_string_an2w], tags),15)


# wordcloud

library(SnowballC)
library(wordcloud)
library(slam)

fine_food_data_corpus <-VCorpus(VectorSource(fine_food_data_selected$Text))

fine_food_data_text_tdm <-TermDocumentMatrix(fine_food_data_corpus,
                          control =list(tolower =TRUE,
                                        removeNumbers =TRUE,
                                        stopwords =TRUE,
                                        removePunctuation =TRUE,
                                        stemming =TRUE
                                        ))

wc_tdm <- rollup(fine_food_data_text_tdm,2,na.rm=TRUE,FUN=sum)
matrix_c <-as.matrix(wc_tdm)
wc_freq <-sort(rowSums(matrix_c))
wc_tmdata <-data.frame(words=names(wc_freq), wc_freq)
wc_tmdata <-na.omit(wc_tmdata)

wordcloud (tail(wc_tmdata$words,100), tail(wc_tmdata$wc_freq,100), random.order=FALSE, colors=brewer.pal(8, "Dark2"))

```


# **Text analysis**

we will introduce you to the powerful world of text analytics by using a third-party API ( (Application Programming Interface) called from within R. We will be using Microsoft Cognitive Services API to show some real-time analysis of text from the Twitter feed of a news agency.

Microsoft Cognitive Services is a machine intelligence service. This service provide a cloud-based APIs for developers to do lot of high-end functions like face recognition, speech recognition, text mining, video feed analysis, and many others. We will be using their free developer service to show some text analytics features like : 
    - **Sentiment analysis**: Sentiment analysis will tell us what kind of emotions the tweets are carrying. The Microsoft API returns a value between 0 and 1, where 1 means highly positive sentiment while 0 means highly negative sentiment.
    - **Topic detection**: What the topic of discussion is a document?
    - **Language detection**: Can you just provide something written and it shows you which language it is?
    - **Summarization**: Can we automatically summarize a big document to make it manageable to read

Exemple : Use twitter to analyse
Attention besoin d'un compte Microsoft cognitive


```{r textanalysis}




##" NEED microsoft account, don't realy work"


# # library("twitteR")
# # See Machine learning with R p 424 to use twitter for text analytics
# 
# #install.packages("mscstexta4r")
# library(mscstexta4r)
# 
# Sys.setenv(MSCS_TEXTANALYTICS_URL ="https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment")
# Sys.setenv(MSCS_TEXTANALYTICS_KEY ="2673988d37f941f89440d665ae6dad9b")
# 
# #Initialize the service
# textaInit()
# 
# # Load Packages
# require(tm)
# require(NLP)
# require(openNLP)
# #Read the Forbes article into R environment
# y <-paste(scan("C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/india_after_independence.txt", what="character",sep=" "),collapse=" ")
# 
# convert_text_to_sentences <-function(text, lang ="en") {
# # Function to compute sentence annotations using the Apache OpenNLP Maxent sentence detector employing the default model for language 'en'.
# sentence_token_annotator <-Maxent_Sent_Token_Annotator(language = lang)
# # Convert text to class String from package NLP
# text <-as.String(text)
# # Sentence boundaries in text
# sentence.boundaries <-annotate(text, sentence_token_annotator)
# # Extract sentences
# sentences <-text[sentence.boundaries]
# # return sentences
# return(sentences)
# }
# 
# # Convert the text into sentences
# article_text =convert_text_to_sentences(y, lang ="en")
# 
# 
# ### SEntiment analysis ### 
# #import tweet
# 
# tweets = read.csv(file="C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/Twitter Feed From TimesNow.csv")
# 
# 
# document_lang <-rep("en", length(tweets$text))
# tweets$text= as.character(tweets$text)
# 
# tryCatch({
#   # Perform sentiment analysis
#   output_1 <-textaSentiment(
#             documents = tweets$text, # Input sentences or documents
#             languages = document_lang
# # "en"(English, default)|"es"(Spanish)|"fr"(French)|"pt"(Portuguese)
#                             )
#         }, error = function(err) {
# # Print error
#             geterrmessage()
#         })
# merged <-output_1$results
# 
# 
# 
# 
# #######
# library(httr)
# library(jsonlite)
# 
# 
# #Setup
# cogapikey<-"2673988d37f941f89440d665ae6dad9b"
# cogapi<-"https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/languages"
# 
# text=c("is this english?"
#        ,"tak er der mere kage"
#        ,"merci beaucoup"
#        ,"guten morgen"
#        ,"bonjour"
#        ,"merde"
#        ,"That's terrible"
#        ,"R is awesome")
# 
# # Prep data
# df<-data_frame(id=1:8,text)
# mydata<-list(documents= df)
# 
# 
# cogapi<-"https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment"
# # Construct a request
# response<-POST(cogapi, 
#                add_headers(`Ocp-Apim-Subscription-Key`=cogapikey),
#                body=toJSON(mydata))
# 
# # Process reponse
# respcontent<-content(response, as="text")
# 
# fromJSON(respcontent)$documents %>%
#    mutate(id=as.numeric(id)) ->
#    responses
```


# **Other topic** 

### Named entity recognition NER
### Optical character recognition OCR
### sentiment analysis
### speech recognition
### topic modeling
