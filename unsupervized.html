<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>unsupervised</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">First Draft</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="manag.html">Data Managment</a>
</li>
<li>
  <a href="visual.html">Data Visualization</a>
</li>
<li>
  <a href="reg.html">Regression</a>
</li>
<li>
  <a href="DT.html">Decission Tree</a>
</li>
<li>
  <a href="neuralneyt.html">Deep Learning</a>
</li>
<li>
  <a href="baye.html">Bayesien</a>
</li>
<li>
  <a href="Unsupervized.html">Unsupervised</a>
</li>
<li>
  <a href="other.html">Other Data analytics</a>
</li>
<li>
  <a href="textmining.html">Text Mining</a>
</li>
<li>
  <a href="TimeS.html">Time Series</a>
</li>
<li>
  <a href="modeval.html">Model Evaluation</a>
</li>
<li>
  <a href="EnsLear.html">Ensemble Learning</a>
</li>
<li>
  <a href="mysql.html">mySQL</a>
</li>
<li>
  <a href="mysql.html">Usefull R tools</a>
</li>
<li>
  <a href="mysql.html">mySQL</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">unsupervised</h1>

</div>


<p>Pas de variable dépendante, découverte des données</p>
<div id="principal-component-analysis" class="section level1">
<h1><strong>Principal component analysis</strong></h1>
<p>L’objectif est de réduire dimension des données pour obtenir une meilleur visualisation. transforme les données mais n’ajoute pas d’information supplémentaire. PCA : transformation linéaire des data d’une dimension à une autre qui conserve un maximum d’information</p>
<p>data : Vecteurs numériques. Si facteurs, transforme en vecteur numérique sinon use FDA</p>
<p>With a fixed number of training samples, the predictive power reduces as the dimensionality increases, and this is known as the Hughes phenomenon</p>
<p>But réduire la variabilité d’un dataset. On crée de nouvelles variables orthogonales qui explique le plus possible de variances des variables.</p>
<p>basé sur la matrice des covariances</p>
<pre class="r"><code>pca &lt;- prcomp(subset(iris, select = -Species))
pca</code></pre>
<pre><code>## Standard deviations:
## [1] 2.0562689 0.4926162 0.2796596 0.1543862
## 
## Rotation:
##                      PC1         PC2         PC3        PC4
## Sepal.Length  0.36138659 -0.65658877  0.58202985  0.3154872
## Sepal.Width  -0.08452251 -0.73016143 -0.59791083 -0.3197231
## Petal.Length  0.85667061  0.17337266 -0.07623608 -0.4798390
## Petal.Width   0.35828920  0.07548102 -0.54583143  0.7536574</code></pre>
<pre class="r"><code>plot(pca)</code></pre>
<p><img src="unsupervized_files/figure-html/PCA-1.png" width="672" /></p>
<pre class="r"><code>mapped_iris &lt;- as.data.frame(predict(pca, iris))
mapped_iris &lt;-cbind(mapped_iris, Species = iris$Species)

ggplot() +
geom_point(data=mapped_iris,aes(x = PC1, y = PC2, colour = Species))</code></pre>
<p><img src="unsupervized_files/figure-html/PCA-2.png" width="672" /></p>
<pre class="r"><code>pca_data &lt;-data[,c(&quot;f381&quot;,&quot;f408&quot;,&quot;f495&quot;,&quot;f529&quot;,&quot;f549&quot;,&quot;f539&quot;,&quot;f579&quot;,&quot;f634&quot;,&quot;f706&quot;,&quot;f743&quot;)]
pca_data &lt;-na.omit(pca_data)

#Normalise the data before applying PCA analysis mean=0, and sd=1
scaled_pca_data &lt;-scale(pca_data)

pca_results &lt;-prcomp(scaled_pca_data)

summary(pca_results)</code></pre>
<pre><code>## Importance of components:
##                           PC1    PC2     PC3     PC4     PC5     PC6
## Standard deviation     1.9931 1.6839 0.97682 0.94634 0.82350 0.58675
## Proportion of Variance 0.3972 0.2836 0.09542 0.08956 0.06781 0.03443
## Cumulative Proportion  0.3972 0.6808 0.77620 0.86576 0.93357 0.96800
##                            PC7     PC8     PC9    PC10
## Standard deviation     0.53722 0.12881 0.11686 0.03351
## Proportion of Variance 0.02886 0.00166 0.00137 0.00011
## Cumulative Proportion  0.99686 0.99852 0.99989 1.00000</code></pre>
<pre class="r"><code>plot(pca_results)</code></pre>
<p><img src="unsupervized_files/figure-html/PCA-3.png" width="672" /></p>
</div>
<div id="cluster-analysis" class="section level1">
<h1><strong>Cluster analysis</strong></h1>
<p>grouper en groupe homogène.Clustering toujours la même chose, le truc qui change est la metrique</p>
<p>Différent type de clustering : - Connectivity models : Distance connectivity between observations is the measure, e.g., hierarchical clustering. - Centroid models : Distance from mean value of each observation/cluster is the measure, e.g., k-means. - Distribution models : Significance of statistical distribution of variables in the dataset is the measure, e.g., expectation maximization algorithms. - Density models: Density in data space is the measure, e.g., DBSCAN models. - Hard Clustering: Each object belongs to exactly one cluster - Soft Clustering : Each object has some likelihood of belonging to a different cluster</p>
<p>Remarque : pas de selection de variables dans le clustering, il faut porter de l’attention sur le dataset et les variables utilisées.</p>
<p>A good clustering algorithm can be evaluated based on two primary objectives: - High intra-class similarity - Low inter-class similarity</p>
<p>Choix de la mesure de similarité important</p>
<pre class="r"><code># introduction to dataset
Data_House_Worth &lt;-read.csv(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/House Worth Data.csv&quot;,header=TRUE);
str(Data_House_Worth)</code></pre>
<pre><code>## &#39;data.frame&#39;:    316 obs. of  5 variables:
##  $ HousePrice   : int  138800 155000 152000 160000 226000 275000 215000 392000 325000 151000 ...
##  $ StoreArea    : num  29.9 44 46.2 46.2 48.7 56.4 47.1 56.7 84 49.2 ...
##  $ BasementArea : int  75 504 493 510 445 1148 380 945 1572 506 ...
##  $ LawnArea     : num  11.22 9.69 10.19 6.82 10.92 ...
##  $ HouseNetWorth: Factor w/ 3 levels &quot;High&quot;,&quot;Low&quot;,&quot;Medium&quot;: 2 3 3 3 3 1 3 1 1 3 ...</code></pre>
<pre class="r"><code>Data_House_Worth$BasementArea &lt;-NULL

ggplot(Data_House_Worth, aes(StoreArea, LawnArea, color = HouseNetWorth))+geom_point()</code></pre>
<p><img src="unsupervized_files/figure-html/cluster-1.png" width="672" /></p>
<div id="hierarchical-clustering" class="section level3">
<h3>Hierarchical Clustering</h3>
<p>Chaque data sont dans un cluster. Les clusters sont aggrégé hiérachiquement en fonction d’une distance la plus faible jusqu’au moment ou il ne reste qu’un cluster</p>
<p>Hierarchical clustering is based on the connectivity model of clusters. The steps involved in the clustering process are: - Start with N clusters,(i.e., assign each element to its own cluster). - Now merge pairs of clusters with the closest to other - Again compute the distance (similarities) and merge with closest one. - Repeat Steps 2 and 3 to exhaust the items until you get all data points in one cluster. - Chose cutoff at how many clusters you want to have.</p>
<pre class="r"><code>library(ggplot2)
library(ggdendro)</code></pre>
<pre><code>## Warning: package &#39;ggdendro&#39; was built under R version 3.3.3</code></pre>
<pre><code>## 
## Attaching package: &#39;ggdendro&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:imager&#39;:
## 
##     label</code></pre>
<pre class="r"><code># Hierachical clustering
clusters &lt;-hclust(dist(Data_House_Worth[,2:3]))
#Plot the dendogram
plot(clusters)</code></pre>
<p><img src="unsupervized_files/figure-html/Hclust-1.png" width="672" /></p>
<pre class="r"><code>#create different number of cluster
clusterCut_2 &lt;-cutree(clusters, 2)
#table the clustering distribution with actual networth
table(clusterCut_2,Data_House_Worth$HouseNetWorth)</code></pre>
<pre><code>##             
## clusterCut_2 High Low Medium
##            1  104 135     51
##            2   26   0      0</code></pre>
<pre class="r"><code>clusterCut_3 &lt;-cutree(clusters, 3)
table(clusterCut_3,Data_House_Worth$HouseNetWorth)</code></pre>
<pre><code>##             
## clusterCut_3 High Low Medium
##            1    0 122      1
##            2  104  13     50
##            3   26   0      0</code></pre>
<pre class="r"><code># ici choix du nombre cluster = 3 par hypothèse sur le business

ggplot(Data_House_Worth, aes(StoreArea, LawnArea, color = HouseNetWorth)) +
geom_point(alpha =0.4, size =3.5) +geom_point(col = clusterCut_3) +
scale_color_manual(values =c(&#39;black&#39;, &#39;red&#39;, &#39;green&#39;))</code></pre>
<p><img src="unsupervized_files/figure-html/Hclust-2.png" width="672" /></p>
<pre class="r"><code>### Iris Example

 iris_dist &lt;- dist(scale(subset(iris, select = -Species)))
 clustering &lt;- hclust(iris_dist)
 plot(clustering)</code></pre>
<p><img src="unsupervized_files/figure-html/Hclust-3.png" width="672" /></p>
<pre class="r"><code> ggdendrogram(clustering) + theme_dendro()</code></pre>
<p><img src="unsupervized_files/figure-html/Hclust-4.png" width="672" /></p>
<pre class="r"><code> clusters = cutree(clustering,k = 3)
 
 data = cbind(mapped_iris, Cluster = clusters)
 
 ggplot() +
 geom_point(data= data, aes(x = PC1, y = PC2,
 shape = Species, colour = Cluster))</code></pre>
<p><img src="unsupervized_files/figure-html/Hclust-5.png" width="672" /></p>
</div>
<div id="k-means-clustering" class="section level3">
<h3>K-means clustering</h3>
<p>K-means place observations into Kclusters by minimizing the wihtin-cluster sum of squares (WCSS). WCSS est la somme des distance entre chaque observation et le centre du cluster</p>
<p>Algorithm : - Assignment: Assign each observation to the cluster that gives the minimum within cluster sum of squares (WCSS). - Update: Update the centroid by taking the mean of all the observation in the cluster. - These two steps are iteratively executed until the assignments in any two consecutive iteration don’t change</p>
<p>To find the optimal value of k, we use and Elbow curve that show percentage of variance explained as a functionof nombrr of cluster</p>
<pre class="r"><code>wss &lt;-(nrow(Data_House_Worth)-1)*sum(apply(Data_House_Worth[,2:3],2,var))

for (i in 2:15) {
    wss[i]&lt;-sum(kmeans(Data_House_Worth[,2:3],centers=i)$withinss)
                }

plot(1:15, wss, type=&quot;b&quot;, xlab=&quot;Number of Clusters&quot;,ylab=&quot;Within groups sum of squares&quot;)</code></pre>
<p><img src="unsupervized_files/figure-html/Kmeans-1.png" width="672" /></p>
<pre class="r"><code># 3 cluster explain most of the variance in data. 4cluster not more interest and not in concordance with intuition

# Model
Cluster_kmean &lt;-kmeans(Data_House_Worth[,2:3], 3, nstart =20)
table(Cluster_kmean$cluster,Data_House_Worth$HouseNetWorth)</code></pre>
<pre><code>##    
##     High Low Medium
##   1    0 122      1
##   2   84   0      0
##   3   46  13     50</code></pre>
<pre class="r"><code>Cluster_kmean$cluster &lt;-factor(Cluster_kmean$cluster)

ggplot(Data_House_Worth, aes(StoreArea, LawnArea, color = HouseNetWorth)) + geom_point(alpha =0.4, size =3.5) +geom_point(col = Cluster_kmean$cluster) +scale_color_manual(values =c(&#39;black&#39;, &#39;red&#39;, &#39;green&#39;))</code></pre>
<p><img src="unsupervized_files/figure-html/Kmeans-2.png" width="672" /></p>
<pre class="r"><code># Capture cluster very well</code></pre>
</div>
<div id="ditribution-based-clustering" class="section level3">
<h3>Ditribution-based clustering</h3>
<p>Distribution methods are iterative methods to fit a set of dataset into clusters by optimizing distributions of datasets in clusters (i.e. Gaussian distribution). - First randomly choose Gaussian parameters and fit it to set of data points. - Iteratively optimize the distribution parameters to fit as many points it can. - Once it converges to a local minima, you can assign data points closer to that distribution of that cluster</p>
<p>Attention cette méthode souffre d’overfitting</p>
<pre class="r"><code>library(EMCluster, quietly =TRUE)</code></pre>
<pre><code>## Warning: package &#39;EMCluster&#39; was built under R version 3.3.3</code></pre>
<pre class="r"><code>#model
ret &lt;-init.EM(Data_House_Worth[,2:3], nclass =3)
ret</code></pre>
<pre><code>## Method: em.EMRnd.EM
##  n = 316, p = 2, nclass = 3, flag = 0, logL = -1871.0335.
## nc: 
## [1] 100  48 168
## pi: 
## [1] 0.2508 0.2000 0.5492</code></pre>
<pre class="r"><code># assign class
ret.new &lt;-assign.class(Data_House_Worth[,2:3], ret, return.all =FALSE)

plotem(ret,Data_House_Worth[,2:3])</code></pre>
<p><img src="unsupervized_files/figure-html/dist%20clust-1.png" width="672" /></p>
<pre class="r"><code>ggplot(Data_House_Worth, aes(StoreArea, LawnArea, color = HouseNetWorth)) +
geom_point(alpha =0.4, size =3.5) +geom_point(col = ret.new$class) +
scale_color_manual(values =c(&#39;black&#39;, &#39;red&#39;, &#39;green&#39;))</code></pre>
<p><img src="unsupervized_files/figure-html/dist%20clust-2.png" width="672" /></p>
<pre class="r"><code># good fort high and low.</code></pre>
</div>
<div id="density-based-clustering-dbscan" class="section level3">
<h3>Density based clustering DBSCAN</h3>
<p>see more on Machine Learning Using R p 349</p>
</div>
</div>
<div id="evaluation-of-clustering" class="section level1">
<h1><strong>Evaluation of clustering</strong></h1>
<div id="internal-evaluation" class="section level3">
<h3>Internal evaluation</h3>
<pre><code>- Dunn Index : the ratio between the minimal intercluster distances to the maximal intracluster distance. But high score
- Silhouette Coefficient : the average distance to elements in the same cluster with the average distance to elements in other clusters. Objects with a high silhouette value are considered well clustered</code></pre>
</div>
<div id="external-evaluation-on-test-set" class="section level3">
<h3>external evaluation on test set</h3>
<pre><code>- Rand index :  similar to classification rate in multi-class classification problems. measures how many items that are returned by the cluster and expert (labeled) are common and how many differ. $RI = \frac{TP+TN}{TP/FP/FN/TN}$ (true positive, true negative,...)
- Jaccard index : measures the overlap of external labels and labels generated by the cluster algorithms. The Jaccard index value varies between 0 and 1, 0 implying no overlap while 1 means identical datasets. $J= \frac{|A \cap B|}{|A \cup B|} = \frac{TP}{TP+FP+FN}</code></pre>
</div>
</div>
<div id="association-rule-mining-algorithms" class="section level1">
<h1><strong>Association Rule Mining Algorithms</strong></h1>
<p>Association rule learning is a method for discovering interesting relations between variables in large databases using some measures of interestingness.</p>
<p>Pratique courante sur les transactional (supermarket, library,…). Pour mettre produit ensemble pourune promo, planning, customer segmentation, …</p>
<p>Usefull measures : - Support : is the proportion of transactions in which an item set appears - Confidence : indicates the strength of a rule. is the conditional probability $conf(X=&gt;Y) =  - Lift : is a ratio between the observed support to the expected support. If = 1 then independent. $ Lift(X=&gt;Y) = </p>
<p>=&gt; more information in Beginning Data Science With R p 192 or Machine learning with R chap 6.10</p>
</div>
<div id="singular-value-decomposition" class="section level1">
<h1><strong>Singular Value decomposition</strong></h1>
</div>
<div id="k-nearest-neighbot" class="section level1">
<h1><strong>K-Nearest Neighbot</strong></h1>
</div>
<div id="learning-vector-quantization" class="section level1">
<h1><strong>Learning Vector Quantization</strong></h1>
</div>
<div id="self-organizing-map-sqm" class="section level1">
<h1><strong>Self-Organizing MAP (SQM)</strong></h1>
</div>
<div id="partitioning-around-medoids-pam" class="section level1">
<h1><strong>Partitioning around Medoids PAM</strong></h1>
</div>
<div id="dimensionality-reduction-algorithms" class="section level1">
<h1><strong>Dimensionality reduction algorithms</strong></h1>
<div id="pca" class="section level3">
<h3>PCA</h3>
</div>
<div id="principal-component-regression" class="section level3">
<h3>principal component regression</h3>
</div>
<div id="partial-least-square-regression" class="section level3">
<h3>Partial least square regression</h3>
</div>
<div id="multidimendional-scaling-mds" class="section level3">
<h3>Multidimendional scaling MDS</h3>
</div>
<div id="linear-discriminant-analysis-lda" class="section level3">
<h3>Linear discriminant Analysis LDA</h3>
</div>
<div id="mixture-discriminant-analysis-mda" class="section level3">
<h3>Mixture discriminant Analysis MDA</h3>
</div>
<div id="quadratic-discriminant-analysis-qda" class="section level3">
<h3>Quadratic discriminant analysis QDA</h3>
</div>
</div>
<div id="exemple-avec-fuzzy-c-means-clustering" class="section level1">
<h1><strong>exemple avec Fuzzy C-Means Clustering</strong></h1>
<p>This is the fuzzy version of the known k-means clustering algorithm as well as an online variant (Unsupervised Fuzzy Competitive learning).</p>
<p>Observe that we are passing the value ucfl to the parameter method, which does an online update of model using Unsupervised Fuzzy Competitive Learning (UCFL)</p>
<p>On suppose que les donn?es ce mettent a jours et a chaque nouvelle observation le modle s’update</p>
<pre class="r"><code>library(e1071)
Data_House_Worth &lt;-read.csv(&quot;C:/Users/007/Desktop/Data science with R/R/Dataset/Chapter 6/House Worth Data.csv&quot;,header=TRUE)

str(Data_House_Worth)</code></pre>
<pre><code>## &#39;data.frame&#39;:    316 obs. of  5 variables:
##  $ HousePrice   : int  138800 155000 152000 160000 226000 275000 215000 392000 325000 151000 ...
##  $ StoreArea    : num  29.9 44 46.2 46.2 48.7 56.4 47.1 56.7 84 49.2 ...
##  $ BasementArea : int  75 504 493 510 445 1148 380 945 1572 506 ...
##  $ LawnArea     : num  11.22 9.69 10.19 6.82 10.92 ...
##  $ HouseNetWorth: Factor w/ 3 levels &quot;High&quot;,&quot;Low&quot;,&quot;Medium&quot;: 2 3 3 3 3 1 3 1 1 3 ...</code></pre>
<pre class="r"><code>Data_House_Worth$BasementArea &lt;-NULL


online_cmean &lt;-cmeans(Data_House_Worth[,2:3],3,20,verbose=TRUE,
                      method=&quot;ufcl&quot;,m=2)</code></pre>
<pre><code>## Iteration:   1, Error: 56.5662750268
## Iteration:   2, Error: 56.0651790159
## Iteration:   3, Error: 55.6463980317
## Iteration:   4, Error: 55.2950527730
## Iteration:   5, Error: 54.9992458983
## Iteration:   6, Error: 54.7494365653
## Iteration:   7, Error: 54.5379506488
## Iteration:   8, Error: 54.3585975488
## Iteration:   9, Error: 54.2063703262
## Iteration:  10, Error: 54.0772108347
## Iteration:  11, Error: 53.9678255467
## Iteration:  12, Error: 53.8755409742
## Iteration:  13, Error: 53.7981901077
## Iteration:  14, Error: 53.7340232617
## Iteration:  15, Error: 53.6816382407
## Iteration:  16, Error: 53.6399259298
## Iteration:  17, Error: 53.6080283353
## Iteration:  18, Error: 53.5853068302
## Iteration:  19, Error: 53.5713189354
## Iteration:  20, Error: 53.5658024371</code></pre>
<pre class="r"><code># print(online_cmean)

ggplot(Data_House_Worth, aes(StoreArea, LawnArea, color = HouseNetWorth)) +
geom_point(alpha =0.4, size =3.5) +geom_point(col = online_cmean$cluster) +
scale_color_manual(values =c(&#39;black&#39;, &#39;red&#39;, &#39;green&#39;))</code></pre>
<p><img src="unsupervized_files/figure-html/F%20Cmeans-1.png" width="672" /></p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
